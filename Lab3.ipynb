{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02HZkdnEzIQy"
      },
      "source": [
        "# Робота з Spark RDD\n",
        "\n",
        "## Найпопулярніші пари продуктів\n",
        "\n",
        "В якості датасету для завдання необхідно використати [Amazon Reviews](https://www.kaggle.com/snap/amazon-fine-food-reviews).\n",
        "\n",
        "> Для більш зручної розробки логіки в додатково в Класрумі є скорочений файл схожої структури `sample.csv`.\n",
        "\n",
        "Датасет має наступну структуру. Друга колонка - ідентифікатор продукту, третя - ідентифікатор юзера:\n",
        "\n",
        "```\n",
        "Id,ProductId,UserId,ProfileName,HelpfulnessNumerator,HelpfulnessDenominator,Score,Time,Summary,Text\n",
        "```\n",
        "\n",
        "Наприклад:\n",
        "\n",
        "| Id | ProductId | UserId | ProfileName | ...інші колонки |\n",
        "|----|-----------|--------|-------------|-----------------|\n",
        "| 1  | B1        | A2     | Patron      | ...             |\n",
        "\n",
        "### Опис завдання\n",
        "\n",
        "1. Зчитати скорочений або повний датасет як RDD. Після читання вирізати лише потрібні колонки: `UserId` та `ProductId`.\n",
        "2. Створити `RDD`, що містить пари (`tuple`) `UserId` та список всіх `ProductId` для всіх продуктів, які придбав цей юзер. В списку повинні бути лише **унікальні** продукти (`ProductId` для одного юзера не повинні повторюватись). Наприклад:\n",
        "\n",
        "```python\n",
        "(\"A1\", [\"B1\", \"B2\", \"B5\"])\n",
        "(\"A2\", [\"B1\", \"B3\", \"B5\"])\n",
        "...\n",
        "```\n",
        "\n",
        "3. Взявши списки продуктів для кожного юзера, отримати всі пари продуктів які він міг купувати разом. Для кожної такої пари створити `tuple` де першим елементом є пара, другим число `1`. Наприклад для попереднього списку:\n",
        "```python\n",
        "(\"B1,B2\", 1)\n",
        "(\"B1,B5\", 1)\n",
        "(\"B2,B5\", 1)\n",
        "(\"B1,B3\", 1)\n",
        "(\"B1,B5\", 1)\n",
        "(\"B3,B5\", 1)\n",
        "...\n",
        "```\n",
        "\n",
        "> Два продукти вважаються придбаними разом, якщо вони обидва з’являються у списку, який ви отримали на попередньому кроці.\n",
        "\n",
        "4. Підрахувати кількість всіх пар продуктів, відсортувати їх за кількістю від найбільшої до найменшої.\n",
        "5. Взяти лише перші `10` пар продуктів та їх кількість. Наприклад:\n",
        "```python\n",
        "(\"B1,B5\", 23495)\n",
        "(\"B2,B5\", 3340)\n",
        "(\"B3,B5\", 217)\n",
        "...\n",
        "```\n",
        "6. Зберегти результат в текстовий файл."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUNYau0izIQ1"
      },
      "source": [
        "## Конфігурація\n",
        "\n",
        "- `number_cores`: Кількість ядер, виділених під Spark\n",
        "- `memory_gb`: Обʼєм оперативної памʼяті, виділеної під Spark (в Гб)\n",
        "- `is_full_dataset`: Читати повний чи скорочений датасет."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hBYwVRcFzIQ2"
      },
      "outputs": [],
      "source": [
        "number_cores = 2\n",
        "memory_gb = 4\n",
        "is_full_dataset = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "57pkvwUjzIQ3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fadc6743-8334-4e88-fde7-7929af477a59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=905e69a3725088c4be559087f8fa6ca28150b5aaea620942b45d186466437323\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "from pyspark import SparkConf, SparkContext\n",
        "import os\n",
        "\n",
        "number_cores = 2\n",
        "memory_gb = 4\n",
        "conf = (\n",
        "    SparkConf()\n",
        "        .setAppName(\"Spark Rdd Task\")\n",
        "        .setMaster(f'local[{number_cores}]')\n",
        "        .set('spark.driver.memory', f'{memory_gb}g')\n",
        ")\n",
        "\n",
        "sc = SparkContext(conf=conf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jO-qP-nzIQ3"
      },
      "source": [
        "## Рішення"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "S2SM6MV9zIQ3"
      },
      "outputs": [],
      "source": [
        "if is_full_dataset:\n",
        "    if not os.path.exists('Reviews.csv'):\n",
        "        sc.stop()\n",
        "        raise Exception(\"\"\"\n",
        "            Download the 'Reviews.csv' file from https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews\n",
        "            and put it in 'input' folder\n",
        "        \"\"\")\n",
        "    else:\n",
        "        inputRdd = sc.textFile(\"Reviews.csv\")\n",
        "else:\n",
        "    inputRdd = sc.textFile(\"sample.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "24Qtz3mFzIQ4"
      },
      "outputs": [],
      "source": [
        "# Видалення рядку заголовку\n",
        "filteredInput = inputRdd.filter(lambda line: line.startswith(\"Id,\") == False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qb4ZEqvyzIQ4"
      },
      "outputs": [],
      "source": [
        "inputRdd = filteredInput \\\n",
        "             .map(lambda line: line.split(\",\")) \\\n",
        "             .map(lambda x: (x[2], x[1]))\n",
        "\n",
        "\n",
        "user_products_rdd = inputRdd.groupByKey().mapValues(lambda x: list(set(x)))\n",
        "\n",
        "\n",
        "product_pairs_rdd = user_products_rdd.flatMapValues(lambda products: [(products[i], products[j]) for i in range(len(products)) for j in range(i + 1, len(products))]) \\\n",
        "                                    .map(lambda x: (tuple(sorted(x[1])), 1)) \\\n",
        "                                    .reduceByKey(lambda x, y: x + y)\n",
        "\n",
        "\n",
        "top_product_pairs = product_pairs_rdd.takeOrdered(10, key=lambda x: -x[1])\n",
        "\n",
        "\n",
        "with open(\"output.txt\", \"w\") as f:\n",
        "    for pair, count in top_product_pairs:\n",
        "        f.write(f\"{pair}, {count}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTPcuZ0IzIQ4"
      },
      "source": [
        "## Зупинка Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pz02BwifzIQ5"
      },
      "outputs": [],
      "source": [
        "sc.stop()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}